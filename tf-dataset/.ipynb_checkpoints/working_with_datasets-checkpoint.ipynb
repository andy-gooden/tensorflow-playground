{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.Dataset\n",
    "\n",
    "<b>tf.data</b> is an API for input pipelines: makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations<br>\n",
    "\n",
    "It has several classes, such as:\n",
    "* <b><i>class Dataset</i></b>: Base class containing methods to create and transform datasets. Also allows you initialize a dataset from data in memory, or from a Python generator.\n",
    "* <b><i>class DatasetSpec</i></b>: Type specification for tf.data.Dataset.\n",
    "* <b><i>class FixedLengthRecordDataset</i></b>: A Dataset of fixed-length records from one or more binary files.\n",
    "* <b><i>class Options</i></b>: Represents options for tf.data.Dataset.\n",
    "* <b><i>class TFRecordDataset</i></b>: A Dataset comprising records from one or more TFRecord files.\n",
    "* <b><i>class TextLineDataset</i></b>: A Dataset comprising lines from one or more text files.\n",
    "\n",
    "This notebook focuses on <b>tf.data.Dataset</b> which is: <br>\n",
    "* Abstraction representing a sequence of \"elements\"\n",
    "* \"elements\" may consist of one or more components (elem could be train example w 2-components: features and label)\n",
    "\t‚ÅÉ\tof any type representable by tf.TypeSpec, including tf.Tensor, tf.data.Dataset, tf.SparseTensor, * tf.RaggedTensor, or tf.TensorArray\n",
    "* creation from data source in mem or files OR as output of some transformation\n",
    "* more performant than feed_dict\n",
    "* base class: allows init dataset from in-mem data or Python generator\n",
    "\n",
    "This notebook has sections demonstrating:\n",
    "* Creation\n",
    "* Iterators\n",
    "* Mods - such as shuffle and batch\n",
    "* End 2 End Examples\n",
    "* Performance\n",
    "\n",
    "<b>Resources:</b><br>\n",
    "API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset <br>\n",
    "Guides - Build Input Pipeline: https://www.tensorflow.org/guide/data <br>\n",
    "Tutorial: https://www.tensorflow.org/tutorials/estimator/linear <br>\n",
    "blogpost: https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html <br>\n",
    "blogpost: https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428 <br>\n",
    "blogpost: https://medium.com/ymedialabs-innovation/how-to-use-dataset-and-iterators-in-tensorflow-with-code-samples-3bb98b6b74ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/andy.gooden/Virtualenv/tfx-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a NUMPY ARRAY\n",
    "x = np.random.sample((100,2))    # make a numpy array\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "for tensorObj in dataset:\n",
    "    print(str(tensorObj.numpy()))\n",
    "\n",
    "# NOTE: could user iterator, but apparently that is deprecated\n",
    "#iter = dataset.make_one_shot_iterator()\n",
    "#el = iter.get_next()\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "    #print(sess.run(el))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TUPLE of NUMPY ARRAYS\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))  #TYPE: (numpy.ndarray, numpy.ndarray)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))            #TYPE: tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a TENSOR \n",
    "tensor = tf.random_uniform([100, 2])                  #TYPE: tensorflow.python.framework.ops.Tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensor)  #TYPE: tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer, feed_dict={ x: data })\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from generator\n",
    "sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])\n",
    "\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "\n",
    "dataset = tf.data.Dataset().batch(1).from_generator(generator,\n",
    "                                           output_types= tf.int64, \n",
    "                                           output_shapes=(tf.TensorShape([None, 1])))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))\n",
    "    print(sess.run(el))\n",
    "    print(sess.run(el))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a CSV file\n",
    "CSV_PATH = './tweets.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next = iter.get_next()\n",
    "print(next) # next is a dict with key=columns names and value=column data\n",
    "inputs, labels = next['text'], next['sentiment']\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    print(sess.run([inputs,labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
      "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
      "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
      "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
      "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
      "\n",
      "   ca        thal  target  \n",
      "0   0       fixed       0  \n",
      "1   3      normal       1  \n",
      "2   2  reversible       0  \n",
      "3   0      normal       0  \n",
      "4   0      normal       0  \n",
      "\n",
      "COLUMNS\n",
      "dict_keys(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n",
      "\n",
      "AGE Column\n",
      "0    63\n",
      "1    67\n",
      "2    67\n",
      "3    37\n",
      "Name: age, dtype: int64\n",
      "WARNING:tensorflow:From <ipython-input-2-00afdef9a49b>:20: DatasetV1.output_classes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_classes(dataset)`.\n",
      "({'age': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'ca': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'chol': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'cp': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'exang': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'fbs': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'oldpeak': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'restecg': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'sex': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'slope': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'thal': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'thalach': <class 'tensorflow.python.framework.ops.Tensor'>,\n",
      "  'trestbps': <class 'tensorflow.python.framework.ops.Tensor'>},\n",
      " <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      "WARNING:tensorflow:From <ipython-input-2-00afdef9a49b>:21: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
      "({'age': tf.int32,\n",
      "  'ca': tf.int32,\n",
      "  'chol': tf.int32,\n",
      "  'cp': tf.int32,\n",
      "  'exang': tf.int32,\n",
      "  'fbs': tf.int32,\n",
      "  'oldpeak': tf.float64,\n",
      "  'restecg': tf.int32,\n",
      "  'sex': tf.int32,\n",
      "  'slope': tf.int32,\n",
      "  'thal': tf.string,\n",
      "  'thalach': tf.int32,\n",
      "  'trestbps': tf.int32},\n",
      " tf.int32)\n",
      "WARNING:tensorflow:From <ipython-input-2-00afdef9a49b>:22: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "({'age': TensorShape([]),\n",
      "  'ca': TensorShape([]),\n",
      "  'chol': TensorShape([]),\n",
      "  'cp': TensorShape([]),\n",
      "  'exang': TensorShape([]),\n",
      "  'fbs': TensorShape([]),\n",
      "  'oldpeak': TensorShape([]),\n",
      "  'restecg': TensorShape([]),\n",
      "  'sex': TensorShape([]),\n",
      "  'slope': TensorShape([]),\n",
      "  'thal': TensorShape([]),\n",
      "  'thalach': TensorShape([]),\n",
      "  'trestbps': TensorShape([])},\n",
      " TensorShape([]))\n",
      "Every feature: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "A batch of ages: [63 67 67 37 41]\n",
      "A batch of targets: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# From Pandas Dataframe or Series\n",
    "\n",
    "# get some data into a Pandas Dataframe\n",
    "URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv(URL)\n",
    "print(dataframe.head())\n",
    "\n",
    "# convert features and labels into SERIES object\n",
    "labels = dataframe.pop('target')              #TYPE: pandas.core.series.Series\n",
    "features = dict(dataframe)                    #TYPE: dict where KEY is column name and VAL is col as pandas.core.series.Series\n",
    "\n",
    "print(\"\\nCOLUMNS\")\n",
    "print(features.keys())\n",
    "print(\"\\nAGE Column\")\n",
    "print(features['age'][0:4])\n",
    "\n",
    "# make dataset from tupple of Pandas Series objs\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(dataset.output_classes)\n",
    "pp.pprint(dataset.output_types)\n",
    "pp.pprint(dataset.output_shapes)\n",
    "\n",
    "# make a small batch for looking at\n",
    "dataset = dataset.batch(5)\n",
    "\n",
    "# now take a batch and look at it\n",
    "for feature_batch, label_batch in dataset.take(1):\n",
    "    print('Every feature:', list(feature_batch.keys()))\n",
    "    print('A batch of ages:', feature_batch['age'].numpy())   #note: the numpy() bit only works w \"eager execution\"\n",
    "    print('A batch of targets:', label_batch.numpy())\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializable iterator to switch between data\n",
    "EPOCHS = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitializable iterator to switch between Datasets\n",
    "EPOCHS = 10\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op) # switch to train dataset\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "    sess.run(test_init_op) # switch to val dataset\n",
    "    print(sess.run([features, labels]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedable iterator to switch between iterators\n",
    "EPOCHS = 10\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create placeholder\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "# create the iterators from the dataset\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "# same as in the doc https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iter = tf.data.Iterator.from_string_handle(\n",
    "    handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "next_elements = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    test_handle = sess.run(test_iterator.string_handle())\n",
    "    \n",
    "    # initialise iterators. In our case we could have used the 'one-shot' iterator instead,\n",
    "    # and directly feed the data insted the Dataset.from_tensor_slices function, but this\n",
    "    # approach is more general\n",
    "    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    \n",
    "    for _ in range(EPOCHS):\n",
    "        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})\n",
    "        print(x, y)\n",
    "        \n",
    "    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mods - shuffle, batch, repeat, map, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BATCHING\n",
    "BATCH_SIZE = 4\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(8):\n",
    "        print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     this will run forever\n",
    "        for _ in range(len(x)):\n",
    "            print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHUFFLE\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End 2 End Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to pass the value to a model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "x, y = iter.get_next()\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Initializable iterator\n",
    "EPOCHS = 10\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "n_batches = train_data[0].shape[0] // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Reinitializable iterator\n",
    "EPOCHS = 10\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = {}\n",
    "# copied form https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "def how_much(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        \n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__)\n",
    "            kw['log_time'][name] = (te - ts)\n",
    "            \n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "import time\n",
    "DATA_SIZE = 5000\n",
    "DATA_SHAPE = ((32,32),(20,))\n",
    "BATCH_SIZE = 64 \n",
    "N_BATCHES = DATA_SIZE // BATCH_SIZE\n",
    "EPOCHS = 10\n",
    "\n",
    "test_size = (DATA_SIZE//100)*20 \n",
    "\n",
    "train_shape = ((DATA_SIZE, *DATA_SHAPE[0]),(DATA_SIZE, *DATA_SHAPE[1]))\n",
    "test_shape = ((test_size, *DATA_SHAPE[0]),(test_size, *DATA_SHAPE[1]))\n",
    "print(train_shape, test_shape)\n",
    "train_data = (np.random.sample(train_shape[0]), np.random.sample(train_shape[1]))\n",
    "test_data = (np.random.sample(test_shape[0]), np.random.sample(test_shape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# used to keep track of the methodds\n",
    "log_time = {}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_shape = [None, *DATA_SHAPE[0]] # [None, 64, 64, 3]\n",
    "output_shape = [None,*DATA_SHAPE[1]] # [None, 20]\n",
    "print(input_shape, output_shape)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=input_shape), tf.placeholder(tf.float32, shape=output_shape)\n",
    "\n",
    "@how_much\n",
    "def one_shot(**kwargs):\n",
    "    print('one_shot')\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE).repeat()\n",
    "    train_el = train_dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE).repeat()\n",
    "    test_el = test_dataset.make_one_shot_iterator().get_next()\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(train_el)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(test_el)\n",
    "            \n",
    "@how_much\n",
    "def initialisable(**kwargs):\n",
    "    print('initialisable')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    elements = iter.get_next()\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "        sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "@how_much            \n",
    "def reinitializable(**kwargs):\n",
    "    print('reinitializable')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    # create a iterator of the correct shape and type\n",
    "    iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "    # create the initialisation operations\n",
    "    train_init_op = iter.make_initializer(train_dataset)\n",
    "    test_init_op = iter.make_initializer(test_dataset)\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        sess.run(train_init_op, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "        sess.run(test_init_op, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "@how_much            \n",
    "def feedable(**kwargs):\n",
    "    print('feedable')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()\n",
    "    # create the iterators from the dataset\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iter = tf.data.Iterator.from_string_handle(\n",
    "        handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    test_handle = sess.run(test_iterator.string_handle())\n",
    "\n",
    "    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements, feed_dict={handle: train_handle})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements, feed_dict={handle: test_handle})\n",
    "            \n",
    "one_shot(log_time=log_time)\n",
    "initialisable(log_time=log_time)\n",
    "reinitializable(log_time=log_time)\n",
    "feedable(log_time=log_time)\n",
    "\n",
    "sorted((value,key) for (key,value) in log_time.items())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
